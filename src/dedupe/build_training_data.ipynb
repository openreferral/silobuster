{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e302a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tld'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45478/3719581001.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostgres_feed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPostgresFeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_org_name\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmangle_org_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_url\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmangle_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtld_swap_prob_dict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtld_swap_prob_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hackathon/microservice/source/silobuster-dedupe/manglers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_org_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtld_swap_prob_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hackathon/microservice/source/silobuster-dedupe/manglers/mangle_url.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtld\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tld\u001b[0m \u001b[0;31m# to get domain extension (aka top-level domain or 'TLD') of URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_org_name\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_remove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_replace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_null\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# consider getting more extensive list of domain extensions, from data or from TLD website\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tld'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "from psycopg2 import connect, extras\n",
    "\n",
    "# from libs.connectors.postgres_connector import PostgresConnector\n",
    "# from libs.feeds.postgres_feed import PostgresFeed\n",
    "\n",
    "from manglers.mangle_org_name import mangle_org_name\n",
    "from manglers.mangle_url import mangle_url\n",
    "from manglers.mangle_address import mangle_address\n",
    "from manglers.mangle_region import mangle_region\n",
    "from manglers.mangle_country import mangle_country\n",
    "import manglers.mangler_parameters\n",
    "\n",
    "# connection details for source data table\n",
    "source_host = 'localhost'\n",
    "source_user = 'postgres'\n",
    "source_passwd = 'postgres'\n",
    "source_db = 'defaultdb'\n",
    "source_port = 5432\n",
    "\n",
    "# connection details for training data table\n",
    "training_host = 'localhost'\n",
    "training_user = 'postgres'\n",
    "training_passwd = 'postgres'\n",
    "training_db = 'silobuster_testing'\n",
    "training_port = 5432\n",
    "training_table = 'organizations_mangled_110422'\n",
    "training_set_num = 1\n",
    "\n",
    "replace_train_data = 'Y'\n",
    "\n",
    "csv_output = 'training_sets/training_set_110422.csv'\n",
    "\n",
    "print ('Starting connections...')\n",
    "source_conn = connect(\n",
    "        database=source_db,\n",
    "        user=source_user,\n",
    "        password=source_passwd,\n",
    "        host=source_host,\n",
    "        port=source_port\n",
    "    )\n",
    "\n",
    "training_conn = connect(\n",
    "        database=training_db,\n",
    "        user=training_user,\n",
    "        password=training_passwd,\n",
    "        host=training_host,\n",
    "        port=training_port\n",
    "    )\n",
    "\n",
    "# defining SQL queries to run to create/replace training table, and inserting original and mangled rows\n",
    "create_qry = f\"\"\"\n",
    "    create table if not exists {training_table} (\n",
    "        id Serial,\n",
    "        name VARCHAR,\n",
    "        description VARCHAR,\n",
    "        url VARCHAR,\n",
    "        address_1 VARCHAR,\n",
    "        address_2 VARCHAR,\n",
    "        city VARCHAR,\n",
    "        state_province VARCHAR,\n",
    "        postal_code VARCHAR,\n",
    "        country VARCHAR,\n",
    "        type VARCHAR,\n",
    "        region VARCHAR,\n",
    "        duplicate_id VARCHAR,\n",
    "        duplicate_type VARCHAR,\n",
    "        training_set VARCHAR\n",
    "    )\n",
    "\"\"\"\n",
    "drop_qry = f\"DROP TABLE IF EXISTS {training_table}\"\n",
    "select_qry = \"select t1.name, t1.description, t1.url, t3.address_1, t3.address_2, t3.city, t3.region, t3.state_province, t3.postal_code, t3.country, t3.type from organization t1 left join location t2 on t1.id = t2.organization_id left join address t3 on t3.location_id = t2.id\"\n",
    "insert_qry = f\"INSERT INTO {training_table} (name, description, url, address_1, address_2, city, state_province, postal_code, country, type, region, training_set) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING id\"\n",
    "insert_dup_qry = f\"INSERT INTO {training_table} (name, description, url, address_1, address_2, city, state_province, postal_code, country, type, region, duplicate_id, duplicate_type, training_set) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "with training_conn.cursor() as training_cur:\n",
    "    if replace_train_data == 'Y':\n",
    "        training_cur.execute(drop_qry)\n",
    "    training_cur.execute(create_qry)\n",
    "    training_conn.commit()\n",
    "\n",
    "print (\"Connected?\")\n",
    "\n",
    "### Data keys for mangling\n",
    "\n",
    "# bringing in comprehensive list of all possible address suffixes\n",
    "# each row of suffixes contains list of possible versions of a specific suffix\n",
    "# e.g. one row has Street,ST,st,st.,Str\n",
    "address_suffixes = []\n",
    "with open('./helper_data/street_suffixes.csv', 'r') as suffixes_file:\n",
    "    csv_reader = csv.reader(suffixes_file)\n",
    "    for row in csv_reader:\n",
    "        row_clean = [suffix for suffix in row if suffix] # removing empty strings\n",
    "        address_suffixes.append(row_clean)\n",
    "\n",
    "state_keys = list()\n",
    "state_keys.append(['wa', 'wash', 'washington'])\n",
    "\n",
    "with source_conn.cursor(cursor_factory=extras.RealDictCursor) as source_cur:\n",
    "    source_cur.execute(select_qry)\n",
    "    data = source_cur.fetchall()\n",
    "    print (\"Retrieved data...\")\n",
    "    \n",
    "    dup_address = random.randint(1,3)\n",
    "    dup_blank_stuff = random.randint(1,2)\n",
    "    \n",
    "    for count, row in enumerate(data):\n",
    "        \n",
    "        print (f\"Insert row count: {count}\")\n",
    "        \n",
    "        # removing trailing/leading spaces and converting to lowercase\n",
    "        name = row['name'].strip().lower()\n",
    "        try:\n",
    "            desc = row['description'].strip().lower()\n",
    "        except:\n",
    "            desc = ''\n",
    "        try:\n",
    "            url = row['url'].strip().lower()\n",
    "        except:\n",
    "            url = ''\n",
    "        try:\n",
    "            address_1 = row['address_1'].strip().lower()\n",
    "        except:\n",
    "            address_1 = ''\n",
    "        try:\n",
    "            address_2 = row['address_2'].strip().lower()\n",
    "        except:\n",
    "            address_2 = ''\n",
    "        try:\n",
    "            city = row['city'].strip().lower()\n",
    "        except:\n",
    "            city = ''\n",
    "        try:\n",
    "            region = row['region'].strip().lower()\n",
    "        except:\n",
    "            region = ''\n",
    "        try:\n",
    "            state = row['state_province'].strip().lower()\n",
    "        except:\n",
    "            state = ''\n",
    "        try:\n",
    "            postal = row['postal_code'].strip().lower()\n",
    "        except:\n",
    "            postal = ''\n",
    "        try:\n",
    "            country = row['country'].strip().lower()\n",
    "        except:\n",
    "            country = ''\n",
    "        try:\n",
    "            type_row = row['type'].strip().lower()\n",
    "        except:\n",
    "            type_row = ''\n",
    "        \n",
    "        # Write the rows to the mangled table\n",
    "        with training_conn.cursor() as training_cur:\n",
    "            training_cur.execute(insert_qry, [\n",
    "                    name, \n",
    "                    desc, \n",
    "                    url, \n",
    "                    address_1, \n",
    "                    address_2, \n",
    "                    city, \n",
    "                    state, \n",
    "                    postal,\n",
    "                    country, \n",
    "                    type_row,\n",
    "                    region,\n",
    "                    training_set_num\n",
    "\n",
    "            ])\n",
    "            insert_id = training_cur.fetchone()[0]\n",
    "            training_conn.commit()\n",
    "        \n",
    "        \n",
    "        with open(csv_output, 'a') as csvfile:\n",
    "            w = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "            w.writerow([\n",
    "                name,\n",
    "                desc,\n",
    "                url,\n",
    "                address_1,\n",
    "                address_2,\n",
    "                city,\n",
    "                state,\n",
    "                postal,\n",
    "                country,\n",
    "                type_row,\n",
    "                region,\n",
    "                '',\n",
    "                '',\n",
    "                training_set_num,\n",
    "            ])\n",
    "\n",
    "        # mangling organization name, region, country, URL, address_1\n",
    "        mangled_name = mangle_org_name(name,  \n",
    "            remove_prob = manglers.mangler_parameters.name_remove_char_prob, \n",
    "            replace_prob = manglers.mangler_parameters.name_replace_char_prob, \n",
    "            null_prob = manglers.mangler_parameters.name_nullify_prob\n",
    "        )\n",
    "        mangled_region = mangle_region(region, \n",
    "            remove_prob = manglers.mangler_parameters.region_remove_char_prob, \n",
    "            replace_prob = manglers.mangler_parameters.region_replace_char_prob, \n",
    "            null_prob = manglers.mangler_parameters.region_nullify_prob\n",
    "        )\n",
    "        mangled_country = mangle_country(country, \n",
    "            remove_prob = manglers.mangler_parameters.country_remove_char_prob, \n",
    "            replace_prob = manglers.mangler_parameters.country_replace_char_prob, \n",
    "            null_prob = manglers.mangler_parameters.country_nullify_prob\n",
    "        )\n",
    "        mangled_url = mangle_url(url, \n",
    "            probs_dict = manglers.mangler_parameters.url_mangling_probs_dict, \n",
    "            tld_swap_prob_dict = manglers.mangler_parameters.tld_swap_prob_dict\n",
    "        )\n",
    "        mangled_address_1 = mangle_address(address_1, \n",
    "            suffixes = address_suffixes, \n",
    "            swap_suffix_prob = manglers.mangler_parameters.address_suffix_swap_prob, \n",
    "            blank_prob = manglers.mangler_parameters.address_nullify_prob\n",
    "        )\n",
    "\n",
    "        # assign tags based on how field(s) were mangled in duplicated row\n",
    "        duplicate_type = []\n",
    "        original_fields = [name, address_1, url, region, country]\n",
    "        mangled_fields = [mangled_name, mangled_address_1, mangled_url, mangled_region, mangled_country]\n",
    "        duplicate_tags = ['mangled_name', 'mangled_address_1', 'mangled_url', 'mangled_region', 'mangled_country']\n",
    "\n",
    "        for orig_field, mangled_field, tag in zip(original_fields, mangled_fields, duplicate_tags):\n",
    "            if orig_field != mangled_field:\n",
    "                duplicate_type.append(tag)\n",
    "        \n",
    "        # if any of the fields were mangled, create a duplicate row in database and in training set CSV file\n",
    "        if len(duplicate_type) > 0:  \n",
    "            dup_row = {\n",
    "                'name': mangled_name,\n",
    "                'description': desc,\n",
    "                'url': mangled_url,\n",
    "                'address_1': mangled_address_1,\n",
    "                'address_2': address_2,\n",
    "                'city': city,\n",
    "                'region': mangled_region,\n",
    "                'state': state,\n",
    "                'postal': postal,\n",
    "                'country': mangled_country,\n",
    "                'duplicate_id': insert_id,\n",
    "                'duplicate_type': duplicate_type,\n",
    "                'training_set': training_set_num\n",
    "            }\n",
    "\n",
    "            # insert duplicated row into training/mangled table\n",
    "            with training_conn.cursor() as dup1_cur:\n",
    "                dup1_cur.execute(insert_dup_qry, [\n",
    "                        mangled_name,\n",
    "                        desc,\n",
    "                        mangled_url,\n",
    "                        mangled_address_1,\n",
    "                        address_2,\n",
    "                        city,\n",
    "                        state,\n",
    "                        postal,\n",
    "                        mangled_country,\n",
    "                        type_row,\n",
    "                        mangled_region,\n",
    "                        insert_id,\n",
    "                        '|'.join(duplicate_type),\n",
    "                        training_set_num,\n",
    "                    ])\n",
    "                \n",
    "    \n",
    "            # write duplicated row to CSV file\n",
    "            with open(csv_output, 'a') as csvfile:\n",
    "                w = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "                w.writerow([\n",
    "                    mangled_name,\n",
    "                    desc,\n",
    "                    mangled_url,\n",
    "                    mangled_address_1,\n",
    "                    address_2,\n",
    "                    city,\n",
    "                    state,\n",
    "                    postal,\n",
    "                    mangled_country,\n",
    "                    type_row,\n",
    "                    mangled_region,\n",
    "                    insert_id,\n",
    "                    '|'.join(duplicate_type),\n",
    "                    training_set_num,\n",
    "                ])\n",
    "                \n",
    " \n",
    "print ('finished')           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085c4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
